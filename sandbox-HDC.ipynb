{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDC Module development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones((1,5,5))\n",
    "kern = torch.ones([1, 1, 3, 3])# * torch.Tensor([1,2,3])\n",
    "kern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [198]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(in_channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, out_channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,dilation\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m c2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(in_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, dilation\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:434\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=431'>432</a>\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=432'>433</a>\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=433'>434</a>\u001b[0m \u001b[39msuper\u001b[39;49m(Conv2d, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=434'>435</a>\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m    <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=435'>436</a>\u001b[0m     \u001b[39mFalse\u001b[39;49;00m, _pair(\u001b[39m0\u001b[39;49m), groups, bias, padding_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:94\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=89'>90</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=90'>91</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInvalid padding string \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m, should be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=91'>92</a>\u001b[0m                 padding, valid_padding_strings))\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=92'>93</a>\u001b[0m     \u001b[39mif\u001b[39;00m padding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(s \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stride):\n\u001b[0;32m---> <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=93'>94</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpadding=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported for strided convolutions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=95'>96</a>\u001b[0m valid_padding_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcircular\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m     <a href='file:///Users/veikko/Documents/GitHub/organ-segmentation/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=96'>97</a>\u001b[0m \u001b[39mif\u001b[39;00m padding_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[0;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "c1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2,dilation=1, padding=(1,1))\n",
    "c2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, dilation=1, padding=\"valid\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "   c1.weight = nn.Parameter(kern)\n",
    "   c2.weight = nn.Parameter(kern)\n",
    "   c1.bias = nn.Parameter(torch.Tensor([0]))\n",
    "   c2.bias = nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "input = torch.rand((1,7,7)).round(decimals=0)#(torch.ones((1,10,10)) * 1)\n",
    "\n",
    "out1 = c1(input)\n",
    "out2 = c2(input)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "input.shape, out1.shape, out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 1., 1., 1.],\n",
      "         [1., 0., 1., 0., 1., 0., 1.],\n",
      "         [1., 0., 0., 0., 1., 1., 0.],\n",
      "         [1., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 0., 0., 1.],\n",
      "         [0., 1., 0., 1., 1., 0., 1.]]])\n",
      "\n",
      "tensor([[[1., 1., 3., 3., 3., 2., 3.],\n",
      "         [3., 1., 5., 2., 5., 2., 4.],\n",
      "         [1., 2., 4., 5., 5., 4., 5.],\n",
      "         [5., 3., 7., 4., 7., 3., 5.],\n",
      "         [1., 3., 4., 5., 5., 4., 5.],\n",
      "         [3., 3., 4., 4., 4., 3., 3.],\n",
      "         [0., 3., 2., 4., 4., 3., 4.]]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "tensor([[[4., 5., 5.],\n",
      "         [7., 4., 7.],\n",
      "         [4., 5., 5.]]], grad_fn=<SqueezeBackward1>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print()\n",
    "print(out1)\n",
    "print()\n",
    "print(out2)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e11b39e258ce4d5d8da63d84a4ce623d390dee4c95f501d4ac42e2eea86ca73b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
