{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import numpy as np # For general mathematical operations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt # For plotting the results\n",
    "from torchsummary import summary # For model summaries\n",
    "from torch.utils.tensorboard import SummaryWriter # For writing into tensorboard\n",
    "import nrrd # For reading and manipulating nrrd files\n",
    "from glob import glob\n",
    "import torch.nn.functional as F\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop focal loss\n",
    "\n",
    "Specifically solve the alpha issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3600000])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([alpha for _ in range(a)], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "[a:=shape*a for shape in orig_input_shape]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2058)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = torch.Tensor(\n",
    "[0.5, 1.0, 4.0, 1.0, 4.0, 4.0, 1.0, 1.0, 3.0, 3.0]\n",
    ")  # TODO: focal loss weights per channels from the paper\n",
    "\n",
    "weights = torch.ones(dims)\n",
    "alpha_transformed = (weights.transpose(1,-1)*alpha).transpose(1,-1).view(-1)\n",
    "\n",
    "\n",
    "gamma = 2.0\n",
    "dims = [2, 10, 20, 30, 30]\n",
    "\n",
    "targets = torch.rand(dims)\n",
    "inputs = (torch.rand(dims) + targets) / 2\n",
    "\n",
    "# Add error specifically to channel 0\n",
    "inputs[0,...] = torch.rand(dims[1:])\n",
    "\n",
    "orig_input_shape = inputs.shape\n",
    "# flatten label and prediction tensors\n",
    "inputs = inputs.view(-1)\n",
    "targets = targets.view(-1)\n",
    "# first compute binary cross-entropy\n",
    "\n",
    "BCE = F.binary_cross_entropy(inputs, targets, weight=weights_transformed, reduction=\"mean\")\n",
    "\n",
    "BCE_EXP = torch.exp(-BCE)\n",
    "focal_loss = (1 - BCE_EXP) ** gamma * BCE\n",
    "\n",
    "\n",
    "focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((2,3,2,2)).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6300, 0.6700],\n",
      "          [0.2500, 0.8200]],\n",
      "\n",
      "         [[0.2700, 0.6600],\n",
      "          [0.8800, 0.7900]],\n",
      "\n",
      "         [[0.2000, 0.2200],\n",
      "          [0.3000, 0.3600]]],\n",
      "\n",
      "\n",
      "        [[[0.3300, 0.8700],\n",
      "          [0.4700, 0.0400]],\n",
      "\n",
      "         [[0.9600, 0.0000],\n",
      "          [0.8200, 0.7600]],\n",
      "\n",
      "         [[0.5200, 0.4400],\n",
      "          [0.5000, 0.4500]]]])\n",
      "tensor([0.6300, 0.6700, 0.2500, 0.8200, 0.2700, 0.6600, 0.8800, 0.7900, 0.2000,\n",
      "        0.2200, 0.3000, 0.3600, 0.3300, 0.8700, 0.4700, 0.0400, 0.9600, 0.0000,\n",
      "        0.8200, 0.7600, 0.5200, 0.4400, 0.5000, 0.4500])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3., 1., 1., 1., 1., 2., 2.,\n",
      "        2., 2., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.Tensor([1,2,3])\n",
    "weights = torch.ones_like(test)\n",
    "\n",
    "weights = (weights.transpose(1, -1)*w).transpose(1,-1)\n",
    "print(test)\n",
    "print(test.view(-1))\n",
    "print(weights.view(-1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e11b39e258ce4d5d8da63d84a4ce623d390dee4c95f501d4ac42e2eea86ca73b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
