{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import numpy as np # For general mathematical operations\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt # For plotting the results\n",
    "from torchinfo import summary # For model summaries\n",
    "from torch.utils.tensorboard import SummaryWriter # For writing into tensorboard\n",
    "import nrrd # For reading and manipulating nrrd files\n",
    "from glob import glob\n",
    "import torch.nn.functional as F\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop focal loss\n",
    "\n",
    "Specifically solve the alpha issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2587)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = torch.Tensor(\n",
    "[0.5, 1.0, 4.0, 1.0, 4.0, 4.0, 1.0, 1.0, 3.0, 3.0]\n",
    ")  # TODO: focal loss weights per channels from the paper\n",
    "\n",
    "\n",
    "gamma = 2.0\n",
    "dims = [2, 10, 20, 30, 30]\n",
    "weights = torch.ones(dims)\n",
    "alpha_transformed = (weights.transpose(1,-1)*alpha).transpose(1,-1).view(-1)\n",
    "\n",
    "targets = torch.rand(dims)\n",
    "inputs = (torch.rand(dims) + targets) / 2\n",
    "\n",
    "# Add error specifically to channel 0\n",
    "inputs[0,...] = torch.rand(dims[1:])\n",
    "\n",
    "orig_input_shape = inputs.shape\n",
    "# flatten label and prediction tensors\n",
    "inputs = inputs.view(-1)\n",
    "targets = targets.view(-1)\n",
    "# first compute binary cross-entropy\n",
    "\n",
    "BCE = F.binary_cross_entropy(inputs, targets, weight=alpha_transformed, reduction=\"mean\")\n",
    "\n",
    "BCE_EXP = torch.exp(-BCE)\n",
    "focal_loss = (1 - BCE_EXP) ** gamma * BCE\n",
    "\n",
    "\n",
    "focal_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then the dice loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organ sizes\n",
      "tensor([0.4998, 0.9498, 0.5006, 0.5009, 0.5001, 0.5003, 0.4999, 0.5005, 0.4996,\n",
      "        0.4988])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0450),\n",
       " (tensor(0.2152),\n",
       "  tensor([0.0909, 1.0000, 0.4243, 0.0910, 0.0909, 0.0910, 0.0910, 0.0910, 0.0909,\n",
       "          0.0908])))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = [2,10, 50, 100, 50]\n",
    "def old(inputs, targets):\n",
    "    channels = inputs.size()[1]\n",
    "    inputs = inputs[:].contiguous().view(-1)\n",
    "    targets = targets[:].contiguous().view(-1)\n",
    "    intersection = (inputs * targets).sum()\n",
    "    dice = ((2.0 * intersection) / (inputs.sum() + targets.sum())) / channels\n",
    "    return dice\n",
    "\n",
    "def new(inputs, targets, return_per_channel=False):\n",
    "    # Compute the elementwise operations p * y and p + y\n",
    "    dice_top = 2 * inputs * targets + 1e-4\n",
    "    dice_bottom = (inputs + targets + 1e-4)\n",
    "    dice = dice_top / dice_bottom\n",
    "    dsc_per_channel = dice.mean(dim=(0,3,2,4))\n",
    "    dsc_avg = dsc_per_channel.mean()\n",
    "\n",
    "    if return_per_channel:\n",
    "        return dsc_avg, dsc_per_channel\n",
    "\n",
    "    return dsc_avg\n",
    "\n",
    "targets = (torch.rand(dims))\n",
    "targets[:,1] *= 10 # Big organ -> 3 times the size\n",
    "targets[:,0] *= 1 # Small organ -> Half the size\n",
    "targets = targets.round(decimals=0)\n",
    "targets[targets > 1] = 1\n",
    "\n",
    "inputs = torch.nn.Softmax(dim=1)(torch.rand_like(targets))\n",
    "\n",
    "good_ch = 1\n",
    "bad_ch = 2\n",
    "\n",
    "basic_error = 1.0\n",
    "good_error =  0.\n",
    "bad_error = 0.3\n",
    "\n",
    "diff = targets - inputs\n",
    "inputs += (1.0-basic_error)*diff # Normal error\n",
    "\n",
    "diff = targets - inputs\n",
    "inputs[:,good_ch] +=(1-good_error)*diff[:, good_ch] # Good channel\n",
    "\n",
    "diff = targets - inputs\n",
    "inputs[:, bad_ch] += (1.0-bad_error)*diff[:, bad_ch] # BAD Channel\n",
    "\n",
    "old_dsc = old(inputs, targets)\n",
    "new_dsc = new(inputs,targets, return_per_channel=True)\n",
    "\n",
    "print(\"Organ sizes\")\n",
    "print((targets == 1).sum(dim=(0,2,3,4)) / (targets.numel()/targets.shape[1]))\n",
    "\n",
    "old_dsc.round(decimals=3), new_dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0969)\n"
     ]
    }
   ],
   "source": [
    "dice_top = 2 * inputs * targets + 1e-4\n",
    "dice_bottom = (inputs + targets + 1e-4)\n",
    "dice = dice_top / dice_bottom\n",
    "dsc_per_channel = dice.mean(dim=(0,3,2,4))\n",
    "dsc_per_channel = dice.mean()\n",
    "#dsc_avg = dsc_per_channel.mean()\n",
    "\n",
    "print(dsc_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(230.9566), tensor(0.5849))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diceloss = 0\n",
    "dims = (18,27,27)\n",
    "targets = torch.rand(dims)\n",
    "inputs = (torch.rand(dims) + targets) / 2\n",
    "\n",
    "p = inputs\n",
    "y = targets\n",
    "diceloss += (2*(p*y)/(p+y)).sum()\n",
    "\n",
    "diceloss /= dims[1]\n",
    "\n",
    "diceloss, forward(inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets\n",
      "Ch 0\n",
      "[[[[1. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 1. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
      "   [1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 1. 0.]]\n",
      "\n",
      "  [[1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
      "   [0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
      "   [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 0. 0. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 0. 1. 1.]]\n",
      "\n",
      "  [[1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 0. 0.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 1. 0. 0. 1. 1. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "   [0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "   [0. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 1. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 0. 1. 0.]]]]\n",
      "Ch 1\n",
      "[[[[1. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 1. 0. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 1. 1. 0. 1.]]\n",
      "\n",
      "  [[0. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "   [0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 1. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 1. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]]\n",
      "\n",
      "  [[1. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 1. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]\n",
      "   [0. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [0. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 1. 0. 0. 1. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 1. 0. 0.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 1. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [1. 1. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1. 0. 1. 0. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "   [1. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n",
      "   [1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      "   [1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]]]]\n",
      "Predictions\n",
      "Ch 0\n",
      "[[[[1. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 1. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
      "   [1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 1. 0.]]\n",
      "\n",
      "  [[1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
      "   [0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
      "   [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 0. 0. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 0. 1. 1.]]\n",
      "\n",
      "  [[1. 0. 1. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 0. 0.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 1. 0. 0. 1. 1. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "   [0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "   [0. 1. 1. 1. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 1. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 0. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "   [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [1. 1. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 0. 1. 0.]]]]\n",
      "Ch 1\n",
      "[[[[1. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 0. 0. 1. 0. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 1. 1. 0. 1.]]\n",
      "\n",
      "  [[0. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "   [0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "   [0. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 1. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 1. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "   [0. 1. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "   [0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 0. 1. 1.]\n",
      "   [1. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]]\n",
      "\n",
      "  [[1. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "   [0. 1. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 1. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "   [1. 0. 1. 1. 0. 1. 0. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 0. 1. 0. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]\n",
      "   [0. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "   [0. 1. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "   [0. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "   [0. 1. 1. 1. 0. 0. 1. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1. 1. 1. 1. 0. 0.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "   [0. 1. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "   [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [1. 1. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
      "   [0. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 1. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
      "   [0. 1. 0. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "   [0. 1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "   [1. 0. 1. 1. 1. 0. 0. 0. 1. 0.]\n",
      "   [1. 1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      "   [0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "   [0. 0. 0. 1. 0. 1. 0. 1. 0. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "   [1. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n",
      "   [1. 1. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "   [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "   [1. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n",
      "   [1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "\n",
      "  [[0. 1. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "   [1. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "   [0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "   [1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "   [1. 1. 1. 1. 0. 1. 0. 0. 1. 1.]\n",
      "   [1. 0. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
      "   [0. 1. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      "   [1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "from src.losses import DiceCoefficient, DiceLoss, CombinedLoss, FocalLoss\n",
    "from copy import deepcopy\n",
    "\n",
    "eps = 1e-4\n",
    "dsc = DiceCoefficient(eps=eps)\n",
    "dsloss = DiceLoss(eps=eps)\n",
    "combined = CombinedLoss(alpha=[1.0], eps=eps)\n",
    "focal = FocalLoss(eps=eps)\n",
    "\n",
    "dim = (1,2,10,10,10)\n",
    "\n",
    "targets = torch.rand(dim).round(decimals=0)\n",
    "inputs = deepcopy(targets)\n",
    "\n",
    "print(\"Targets\")\n",
    "print(\"Ch 0\")\n",
    "print(targets[:,0].numpy())\n",
    "print(\"Ch 1\")\n",
    "print(targets[:,1].numpy())\n",
    "\n",
    "\n",
    "print(\"Predictions\")\n",
    "print(\"Ch 0\")\n",
    "print(inputs[:,0].numpy())\n",
    "print(\"Ch 1\")\n",
    "print(inputs[:,1].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dice coefficient + mean\n",
      "0.3032807\n",
      "[0.29969332 0.30686808]\n",
      "Dice loss\n",
      "0.6967193\n",
      "[0.29969332 0.30686808]\n",
      "Focal\n",
      "2.265977\n",
      "Combined\n",
      "2.962696\n",
      "[0.29969332 0.30686808]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs[0,0,0,:,:] = 1 - targets[0,0,0,:,:]\n",
    "\n",
    "inputs = torch.rand_like(targets)\n",
    "\n",
    "#inputs[0,0,0,:,:] = targets[0,0,0,:,:]\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Dice coefficient + mean\")\n",
    "coeff, per_ch = dsc(targets, inputs, return_per_channel_dsc=True)\n",
    "print(coeff.numpy())\n",
    "print(per_ch.numpy())\n",
    "\n",
    "print(\"Dice loss\")\n",
    "loss, per_ch = dsloss(targets, inputs, return_per_channel_dsc=True)\n",
    "print(loss.numpy())\n",
    "print(per_ch.numpy())\n",
    "\n",
    "print(\"Focal\")\n",
    "focal_loss = focal(targets, inputs, alpha=torch.Tensor([1.0]))\n",
    "print(focal_loss.numpy())\n",
    "\n",
    "\n",
    "print(\"Combined\")\n",
    "combined_loss, per_ch = combined(targets, inputs, return_per_channel_dsc=True)\n",
    "print(combined_loss.numpy())\n",
    "print(per_ch.numpy())\n",
    "\n",
    "inputs = deepcopy(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(488) tensor(488) tensor(1.)\n",
      "tensor(518) tensor(518) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "n_correct_ch_zero = ((targets[0,0].view(-1) == 1) & (targets[0,0].view(-1) == inputs[0,0].view(-1))).sum()\n",
    "n_correct_ch_one = ((targets[0,1].view(-1) == 1) & (targets[0,1].view(-1) == inputs[0,1].view(-1))).sum()\n",
    "\n",
    "print(n_correct_ch_zero, (targets[0,0].view(-1) == 1).sum(),n_correct_ch_zero / (targets[0,0].view(-1) == 1).sum())\n",
    "print(n_correct_ch_one,(targets[0,1].view(-1) == 1).sum(), n_correct_ch_one / (targets[0,1].view(-1) == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dice coefficient + mean\n",
      "-74.65084\n",
      "[   1.      -150.30168]\n",
      "Dice loss\n",
      "75.65084\n",
      "[   1.      -150.30168]\n",
      "Focal\n",
      "nan\n",
      "Combined\n",
      "nan\n",
      "[   1.      -150.30168]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = deepcopy(targets)\n",
    "\n",
    "inputs[0,0,0,:,:] = 1 - targets[0,0,0,:,:]\n",
    "inputs[0,1,0,:,:] = 1 - targets[0,1,0,:,:]\n",
    "\n",
    "inputs = torch.rand_like(targets)\n",
    "inputs[0,0,:,:,:] = targets[0,0,:,:,:]\n",
    "inputs[0,0,:,:,:] = targets[0,0,:,:,:]\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Dice coefficient + mean\")\n",
    "coeff, per_ch = dsc(targets, inputs, return_per_channel_dsc=True)\n",
    "print(coeff.numpy())\n",
    "print(per_ch.numpy())\n",
    "\n",
    "print(\"Dice loss\")\n",
    "loss, per_ch = dsloss(targets, inputs, return_per_channel_dsc=True)\n",
    "print(loss.numpy())\n",
    "print(per_ch.numpy())\n",
    "alpha_ch0 = -1.0\n",
    "alpha_ch1 = 1.0\n",
    "print(\"Combined\")\n",
    "combined = CombinedLoss(alpha=[alpha_ch0, alpha_ch1])\n",
    "combined_loss, per_ch = combined(targets, inputs, return_per_channel_dsc=True)\n",
    "print(combined_loss.numpy())\n",
    "print(per_ch.numpy())\n",
    "\n",
    "\n",
    "print(\"Focal\")\n",
    "focal_loss = focal(targets, inputs, alpha=torch.Tensor([alpha_ch0, alpha_ch1]*(1000)))\n",
    "print(focal_loss.numpy())\n",
    "\n",
    "\n",
    "\n",
    "#inputs = deepcopy(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1.,  ..., 2., 1., 2.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([alpha_ch0, 2.0]*(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:,0] = 0\n",
    "targets[:,1] = -1\n",
    "\n",
    "targets.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1.,  ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.get_alpha(targets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e11b39e258ce4d5d8da63d84a4ce623d390dee4c95f501d4ac42e2eea86ca73b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
